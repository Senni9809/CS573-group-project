# -*- coding: utf-8 -*-
"""TranditionalML_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dKZwoxm7hAkLVnfvqVBkfsNXKd-bScMU
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install XGBoost (if not installed)
#!pip install xgboost

# Import libraries
import os
import pickle
import numpy as np
from scipy.signal import resample

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

import matplotlib.pyplot as plt

# Define functions to load and preprocess WESAD data
def load_subject_data(subject_id, base_path="/content/drive/MyDrive/Colab Notebooks/Purdue Coursework/CS573_GroupProject/WESAD"):
    subject_path = os.path.join(base_path, f"S{subject_id}", f"S{subject_id}.pkl")
    with open(subject_path, 'rb') as file:
        data = pickle.load(file, encoding='latin1')
    return data

def extract_wrist_data(data):
    wrist_data = data['signal']['wrist']
    target_length = len(wrist_data['BVP'])

    acc_resampled = resample(wrist_data['ACC'], target_length, axis=0)
    eda_resampled = resample(wrist_data['EDA'], target_length).reshape(-1, 1)
    temp_resampled = resample(wrist_data['TEMP'], target_length).reshape(-1, 1)
    bvp = wrist_data['BVP'].reshape(-1, 1)

    X = np.concatenate([acc_resampled, eda_resampled, temp_resampled, bvp], axis=1)
    y = data['label']

    min_len = min(len(X), len(y))
    X = X[:min_len]
    y = y[:min_len]

    # Z-score normalization channel-wise
    for i in range(X.shape[1]):
        X[:, i] = (X[:, i] - np.mean(X[:, i])) / np.std(X[:, i])

    return X, y

# Sliding window function
def segment_data(X, y, window_size=160, step_size=80):
    X_windows, y_windows = [], []

    for start in range(0, len(X) - window_size, step_size):
        end = start + window_size
        window = X[start:end]
        label_window = y[start:end]

        # Majority vote for labels
        labels, counts = np.unique(label_window, return_counts=True)
        majority_label = labels[np.argmax(counts)]

        # Binary mapping: 2 → stress (1), 0/3 → non-stress (0)
        if majority_label in [0, 1, 3]:
            y_final = 0 if majority_label in [0, 3] else 1
            X_windows.append(window)
            y_windows.append(y_final)

    return np.array(X_windows), np.array(y_windows)

# Load and segment all subjects
subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]
X_all, y_all = [], []

for subject_id in subject_ids:
    try:
        data = load_subject_data(subject_id)
        X, y = extract_wrist_data(data)
        X_seg, y_seg = segment_data(X, y)

        if len(X_seg) > 0:
            X_all.append(X_seg)
            y_all.append(y_seg)
        print(f"Loaded subject {subject_id}: {X_seg.shape[0]} windows")

    except Exception as e:
        print(f"Skipping subject {subject_id}: {e}")

# Stack all subjects
X_all = np.concatenate(X_all, axis=0)  # shape: (samples, 160, 4)
y_all = np.concatenate(y_all, axis=0)
print("Final X shape:", X_all.shape)
print("Final y shape:", y_all.shape)

# Flatten time-series for ML input
n_samples, timesteps, n_channels = X_all.shape
X_flat = X_all.reshape(n_samples, -1)  # (samples, 160x4)

# Train/Test Split (Stratified)
X_train, X_test, y_train, y_test = train_test_split(
    X_flat, y_all, test_size=0.2, random_state=42, stratify=y_all
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

# Define the directory to save preprocessed data
save_dir = "/content/drive/MyDrive/Colab Notebooks/Purdue Coursework/CS573_GroupProject/preprocessed_data"
# os.makedirs(save_dir, exist_ok=True)  # Create the directory if it does not exist

# Save the preprocessed train/test splits
with open(os.path.join(save_dir, 'X_train.pkl'), 'wb') as f:
    pickle.dump(X_train, f)

with open(os.path.join(save_dir, 'X_test.pkl'), 'wb') as f:
    pickle.dump(X_test, f)

with open(os.path.join(save_dir, 'y_train.pkl'), 'wb') as f:
    pickle.dump(y_train, f)

with open(os.path.join(save_dir, 'y_test.pkl'), 'wb') as f:
    pickle.dump(y_test, f)

print("Saved preprocessed data successfully.")

