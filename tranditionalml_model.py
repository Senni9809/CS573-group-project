# -*- coding: utf-8 -*-
"""TranditionalML_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bhz0bg4fSwTaSia3rMANjyLv2HmLiyqB
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Import libraries
import os
import pickle
import numpy as np
from scipy.signal import resample
import pandas as pd

from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from scipy.stats import ttest_rel
import seaborn as sns

import matplotlib.pyplot as plt

# Load the preprocessed train/test splits
save_dir = "/content/drive/MyDrive/Colab Notebooks/Purdue Coursework/CS573_GroupProject/preprocessed_data"

with open(os.path.join(save_dir, 'X_train.pkl'), 'rb') as f:
    X_train = pickle.load(f)

with open(os.path.join(save_dir, 'X_test.pkl'), 'rb') as f:
    X_test = pickle.load(f)

with open(os.path.join(save_dir, 'y_train.pkl'), 'rb') as f:
    y_train = pickle.load(f)

with open(os.path.join(save_dir, 'y_test.pkl'), 'rb') as f:
    y_test = pickle.load(f)

print("Loaded preprocessed data successfully.")

# Exploratory Data Analysis(EDA)

# Merge train and test sets for EDA
X_all = np.concatenate((X_train, X_test), axis=0)
y_all = np.concatenate((y_train, y_test), axis=0)

# Stress (1) vs Non-stress (0) counts
unique, counts = np.unique(y_all, return_counts=True)
label_counts = dict(zip(unique, counts))

# Plot
plt.figure(figsize=(6,4))
plt.bar(['Non-Stress (0)', 'Stress (1)'], counts, color=['skyblue', 'salmon'])
plt.title('Label Distribution (Stress vs Non-Stress)')
plt.ylabel('Number of Samples')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# EDA 2: Main feature statistics (Mean, Std)
n_samples, features = X_all.shape

# For each label, calculate mean and std
df = pd.DataFrame(X_all)
df['label'] = y_all

# Group by label (0=Non-stress, 1=Stress)
feature_stats = df.groupby('label').agg(['mean', 'std'])

# Show a few sample statistics
feature_stats.iloc[:, :10]  # First 10 features for display

# EDA 3: Boxplot for a selected feature
# Example: Pick feature 0 (could be a BVP or EDA-related feature)
feature_idx = 0  # Change this if needed

plt.figure(figsize=(6,4))
plt.boxplot([df[df['label'] == 0][feature_idx], df[df['label'] == 1][feature_idx]], labels=['Non-Stress', 'Stress'])
plt.title(f'Boxplot of Feature {feature_idx} (Stress vs Non-Stress)')
plt.ylabel('Feature Value')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)

print("\n=== Random Forest Evaluation ===")
print(classification_report(y_test, rf_pred, digits=4))
print("ROC-AUC:", roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1]))
print("Confusion Matrix:\n", confusion_matrix(y_test, rf_pred))

# Predict on test set
rf_pred = rf.predict(X_test)

# Compute confusion matrix
rf_cm = confusion_matrix(y_test, rf_pred)

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()

# Predict probabilities
rf_probs = rf.predict_proba(X_test)[:, 1]

# Compute ROC curve and AUC
rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)
rf_auc = roc_auc_score(y_test, rf_probs)

# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(rf_fpr, rf_tpr, label=f'AUC = {rf_auc:.3f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title('Random Forest ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# Feature Importance Analysis
importances_rf = rf.feature_importances_
# Top 10 important features (Random Forest)
indices_rf = np.argsort(importances_rf)[-10:][::-1]

plt.figure(figsize=(8,6))
plt.bar(range(len(indices_rf)), importances_rf[indices_rf], align='center')
plt.xticks(range(len(indices_rf)), indices_rf)
plt.xlabel('Feature Index')
plt.ylabel('Importance Score')
plt.title('Top 10 Important Features (Random Forest)')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# 5-fold Cross-validation (Random Forest)
rf_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')
print("=== Random Forest 5-Fold Cross-Validation ===")
print(f"Fold Accuracies: {np.round(rf_scores, 4)}")
print(f"Mean Accuracy: {np.round(np.mean(rf_scores), 4)}")
print(f"Std Dev: {np.round(np.std(rf_scores), 4)}\n")

# XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train, y_train)
xgb_pred = xgb.predict(X_test)

print("\n=== XGBoost Evaluation ===")
print(classification_report(y_test, xgb_pred, digits=4))
print("ROC-AUC:", roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1]))
print("Confusion Matrix:\n", confusion_matrix(y_test, xgb_pred))

# Predict on test set
xgb_pred = xgb.predict(X_test)

# Compute confusion matrix
xgb_cm = confusion_matrix(y_test, xgb_pred)

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Greens', cbar=False)
plt.title('XGBoost Confusion Matrix')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()

# Predict probabilities
xgb_probs = xgb.predict_proba(X_test)[:, 1]

# Compute ROC curve and AUC
xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_probs)
xgb_auc = roc_auc_score(y_test, xgb_probs)

# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(xgb_fpr, xgb_tpr, label=f'AUC = {xgb_auc:.3f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title('XGBoost ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# Feature Importance Analysis
importances_xgb = xgb.feature_importances_
# Top 10 important features (XGBoost)
indices_xgb = np.argsort(importances_xgb)[-10:][::-1]

plt.figure(figsize=(8,6))
plt.bar(range(len(indices_xgb)), importances_xgb[indices_xgb], align='center', color='green')
plt.xticks(range(len(indices_xgb)), indices_xgb)
plt.xlabel('Feature Index')
plt.ylabel('Importance Score')
plt.title('Top 10 Important Features (XGBoost)')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

xgb_scores = cross_val_score(xgb, X_train, y_train, cv=5, scoring='accuracy')
print("=== XGBoost 5-Fold Cross-Validation ===")
print(f"Fold Accuracies: {np.round(xgb_scores, 4)}")
print(f"Mean Accuracy: {np.round(np.mean(xgb_scores), 4)}")
print(f"Std Dev: {np.round(np.std(xgb_scores), 4)}")



# Random Forest 5-Fold Cross-validation
#rf_cv_scores = cross_val_score(rf, X_flat, y_all, cv=5, scoring='accuracy')

# XGBoost 5-Fold Cross-validation
#xgb_cv_scores = cross_val_score(xgb, X_flat, y_all, cv=5, scoring='accuracy')

# 5-Fold Cross-validation results
rf_cv_scores = np.array([0.9711, 0.9711, 0.9735, 0.9735, 0.9727])
xgb_cv_scores = np.array([0.9534, 0.9528, 0.9566, 0.9539, 0.9544])

# Paired t-test
t_stat, p_val = ttest_rel(rf_cv_scores, xgb_cv_scores)

print("=== 5-Fold CV Accuracy ===")
print(f"Random Forest mean accuracy: {np.mean(rf_cv_scores):.4f}")
print(f"XGBoost mean accuracy: {np.mean(xgb_cv_scores):.4f}\n")

print("=== Paired t-test ===")
print(f"t-statistic: {t_stat:.4f}")
print(f"p-value: {p_val:.4e}")

summary_table = pd.DataFrame({
    "Model": ["Random Forest", "XGBoost"],
    "Mean CV Accuracy": [np.mean(rf_cv_scores), np.mean(xgb_cv_scores)],
    "Std Dev": [np.std(rf_cv_scores), np.std(xgb_cv_scores)]
})

print("\n=== Summary Table ===")
print(summary_table)

# EDA-only Random Forest Training and Evaluation
# Extract only EDA signal from flattened data
# In flattened data, EDA appears at every 6th feature starting at index 3
X_train_eda = X_train[:, 3::6]
X_test_eda = X_test[:, 3::6]

# Random Forest model
rf_eda = RandomForestClassifier(n_estimators=100, random_state=42)
rf_eda.fit(X_train_eda, y_train)

# Evaluation
y_pred_eda = rf_eda.predict(X_test_eda)
probs_eda = rf_eda.predict_proba(X_test_eda)[:, 1]

print("=== EDA-only Random Forest Evaluation ===")
print(classification_report(y_test, y_pred_eda, digits=4))
print("ROC-AUC:", roc_auc_score(y_test, probs_eda))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_eda))

# Plot ROC Curve
fpr_eda, tpr_eda, _ = roc_curve(y_test, probs_eda)
roc_auc_eda = roc_auc_score(y_test, probs_eda)

plt.figure(figsize=(6,5))
plt.plot(fpr_eda, tpr_eda, label=f'AUC = {roc_auc_eda:.3f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title('EDA-only Random Forest ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid(True)
plt.show()

# Optional: 5-Fold Cross-validation using full dataset
X_all_full = np.concatenate((X_train, X_test), axis=0)
y_all_full = np.concatenate((y_train, y_test), axis=0)
eda_only_full = X_all_full[:, 3::6]

rf_eda_cv_scores = cross_val_score(RandomForestClassifier(n_estimators=100, random_state=42), eda_only_full, y_all_full, cv=5, scoring='accuracy')

print("\n=== 5-Fold CV (EDA-only Random Forest) ===")
print(f"Fold Accuracies: {np.round(rf_eda_cv_scores, 4)}")
print(f"Mean Accuracy: {np.round(np.mean(rf_eda_cv_scores), 4)}")
print(f"Std Dev: {np.round(np.std(rf_eda_cv_scores), 4)}")

# XGBoost model
xgb_eda = XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb_eda.fit(X_train_eda, y_train)

# Evaluation
y_pred_eda_xgb = xgb_eda.predict(X_test_eda)
probs_eda_xgb = xgb_eda.predict_proba(X_test_eda)[:, 1]

print("=== EDA-only XGBoost Evaluation ===")
print(classification_report(y_test, y_pred_eda_xgb, digits=4))
print("ROC-AUC:", roc_auc_score(y_test, probs_eda_xgb))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_eda_xgb))

# Plot ROC Curve
fpr_eda_xgb, tpr_eda_xgb, _ = roc_curve(y_test, probs_eda_xgb)
roc_auc_eda_xgb = roc_auc_score(y_test, probs_eda_xgb)

plt.figure(figsize=(6,5))
plt.plot(fpr_eda_xgb, tpr_eda_xgb, label=f'AUC = {roc_auc_eda_xgb:.3f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title('EDA-only XGBoost ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid(True)
plt.show()

# 5-Fold Cross-validation
xgb_eda_cv_scores = cross_val_score(xgb_eda, eda_only_full, y_all_full, cv=5, scoring='accuracy')

print("\n=== 5-Fold CV (EDA-only XGBoost) ===")
print(f"Fold Accuracies: {np.round(xgb_eda_cv_scores, 4)}")
print(f"Mean Accuracy: {np.round(np.mean(xgb_eda_cv_scores), 4)}")
print(f"Std Dev: {np.round(np.std(xgb_eda_cv_scores), 4)}")

